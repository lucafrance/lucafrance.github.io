---
title: AI safety matters because humanity matters
tags: [AI]
---

TL, DR: Nicky Case [explains well](https://aisafety.dance) why AI safety matters.

[Nicky Case](https://ncase.me/) recently released the final part of her [AI safety series](https://aisafety.dance). What I appreciate most about the series is how she clearly frames AI safety as both a technological and a human problem.

{:refdef: style="text-align: center;"}
![How can we align AI with humane values?](/assets/2025/ai-safety/breakdown.png){: width="70%" }<br>
*[AI Safety for Fleshy Humans - part 3](https://aisafety.dance/p3)*
{: refdef}

While complex, the technological aspect is perhaps easier to grasp. Large language models (LLMs) are already misaligned in ways previously theorized for artificial general intelligence (AGI)[^nbostrom-superintelligence].
For example, there is experimental evidence that LLMs may pretend to be safe during training only to act maliciously in production[^deceptive-alignment].
Even if AGI is unachievable, LLMs have made AI safety a pressing concern today[^rmiles-AI-ruined].

{:refdef: style="text-align: center;"}
![xkcd comic 1968 - Robot future](/assets/2025/ai-safety/robot_future.png)<br>
*[xkcd 1968 - Robot future](https://xkcd.com/1968)*
{: refdef}

[^nbostrom-superintelligence]: A great book about the AI control problem is [*Superintelligence* by Nick Bostrom (2014)](https://global.oup.com/academic/product/superintelligence-9780199678112)

[^rmiles-AI-ruined]: For a candid perspective on the shift in AI research, check out the video [*AI Ruined My Year* by Robert Miles (YouTube)](https://www.youtube.com/watch?v=2ziuPUeewK0).

[^deceptive-alignment]: This is known as [deceptive alignment](https://aisafety.info/questions/8EL6/What-is-deceptive-alignment), also informally called *volkswagening*. It is described in the paper *Alignment faking in large language models* ([arXiv:2412.14093](https://arxiv.org/abs/2412.14093)). One of the authors discussed it in this video: *[AI Will Try to Cheat & Escape (aka Rob Miles was Right!) - Computerphile (YouTube)](https://www.youtube.com/watch?v=AqJnK9Dh-eQ)*.

The human problem is more insidious than the technological one. Even with the technology to perfectly align AI, what happens if humanity can't agree on what it should do? AI is already deployed in [military applications](https://www.palantir.com/), where reasonable people hold vastly different ethical views.

<div style="text-align: center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/XEM5qz__HOU?si=B_p0FNM_AVioCUuM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

My main takeaway from the series is a stronger awareness of what to prioritize. Any decision about technology must start with its potential benefit to humanity. In the realm of AI tools, I refuse to abandon discussions on AI policy in favour of unchecked technological progress. If political and market incentives aren't aligned in the right direction, it's our responsibility to guide them accordingly.

> A note of pessimism, followed by cautious optimism.
>
> Consider the last few decades in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? "Humans coordinating to deal with global threats" is... not a thing we seem to be good at.
>
> But we used to be good at it! We eradicated smallpox, it's no longer the case that *half* of kids died before age 15, the ozone layer *is* actually > healing!
>
> Humans have solved the "human alignment problem" before.
>
> Let's get our groove back, and align ourselves on aligning AI.
> 
> [AI Safety for Fleshy Humans - part 3](https://aisafety.dance/p3)

## Footnotes
