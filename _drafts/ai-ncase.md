---
title: AI safety for fleshy humans by Nicky Case
tags: [AI]
---

Title? AI safety matters because humanity matters.

TL, DR: Nicky Case [explains well](https://aisafety.dance) why you should care about AI safety.

[Nicky Case](https://ncase.me/) recently released the last part of her [AI safety series](https://aisafety.dance).
What I liked most about the series, is how she clearly frames AI safety as being both a technological and human problem.
https://aisafety.dance/media/p3/intro/breakdown.png

While complex, I believe the technological problem is the easier to grasp.
The intelligent system will only execute what is instructed to do, even if the instructions are poorly formulated.
LLM are already misaligned in ways which were already theorised for artificial general intelligence (AGI)[^nbostrom-superintelligence].
E.g., an LLM can actively mislead humans to appear align and act evil in production.
TODO tecnologia studio AI su cui Robert Miles ha fatto anche un video in cui la LLM attivamente cerca di ingannare l'utente
Even if AGI is never achieved these problems are now real with LLMs with consequences in the real world (TODO example conseuquences)[^rmiles-AI-ruined].

[^nbostrom-superintelligence]: A great book about the AI control problem is [*Superintelligence* by Nick Bostrom (2014)](https://global.oup.com/academic/product/superintelligence-9780199678112)

[^rmiles-AI-ruined]: For a very honest take about the change of perspective on AI research, I recommend the Youtube video [*AI Ruined My Year* by Robert Miles](https://www.youtube.com/watch?v=2ziuPUeewK0).

Even more insidious than the technological problem, there is the human problem.
Even if we have the technology to perfectly align the AI with what to do, what if humanity can't agree on what it should do?
E.g. AI is already seeing in military applications, where humans can have very different views on what proper use is.
TODO human: Demo (di Palanatir?) per una AI per comando di attacchi militari. Veniva menzionato credo nel video di ordinary things sui droni.


What I take most from reading the series is additional awareness about what to focus on.
The starting point for any decision about technology is how it can be good for humanity.
In the context of AI tools, I don't want to give up the discussion about AI policy in name ot technological development for the sake of it.
If the political and market incentives are not going in the right direction, then it is up to us to steer them properly.

> A note of pessimism, followed by cautious optimism.
> 
Consider the last few decades in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? "Humans coordinating to deal with global threats" is... not a thing we seem to be > good at.
> 
> But we used to be good at it! We eradicated smallpox[83], it's no longer the case that half of kids died before age 15[84], the ozone layer is actually > healing! [85]
> 
> Humans have solved the "human alignment problem" before.
> 
> Let's get our groove back, and align ourselves on aligning AI.

- https://aisafety.dance/media/p3/SUM.png

