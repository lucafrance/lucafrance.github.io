---
title: AI safety matters because humanity matters
tags: [AI]
---

TL, DR: Nicky Case [explains well](https://aisafety.dance) why AI safety matters.

[Nicky Case](https://ncase.me/) recently released the last part of her [AI safety series](https://aisafety.dance).
What I liked most about the series, is how she clearly frames AI safety as being both a technological and human problem.

{:refdef: style="text-align: center;"}
![How can we align AI with humane values?](/assets/2025/ai-safety/breakdown.png){: width="70%" }<br>
*[AI Safety for Fleshy Humans - part 3](https://aisafety.dance/p3)*
{: refdef}

While complex, I believe the technological problem is the easier to grasp.
LLM are already misaligned in ways which were theorised for artificial general intelligence (AGI)[^nbostrom-superintelligence].
For example there is experimental evidence of LLMs pretending to be safe during training to act maliciously in production[^deceptive-alignment].
Even if AGI is never achieved, LLMs turned AI safety in a concrete concern today.[^rmiles-AI-ruined].

{:refdef: style="text-align: center;"}
![xkcd comic 1968 - Robot future](/assets/2025/ai-safety/robot_future.png)<br>
*[xkcd 1968 - Robot future](https://xkcd.com/1968)*
{: refdef}

[^nbostrom-superintelligence]: A great book about the AI control problem is [*Superintelligence* by Nick Bostrom (2014)](https://global.oup.com/academic/product/superintelligence-9780199678112)

[^rmiles-AI-ruined]: For a very honest take about the change of perspective on AI research, I recommend the video [*AI Ruined My Year* by Robert Miles (YouTube)](https://www.youtube.com/watch?v=2ziuPUeewK0).

[^deceptive-alignment]: This is known as [deceptive alignment](https://aisafety.info/questions/8EL6/What-is-deceptive-alignment), also informally called *volkswagening*. It is described in the paper *Alignment faking in large language models* ([arXiv:2412.14093](https://arxiv.org/abs/2412.14093)). One of the authors spoke about it in this video: *[AI Will Try to Cheat & Escape (aka Rob Miles was Right!) - Computerphile (youtube)](https://www.youtube.com/watch?v=AqJnK9Dh-eQ)*.

Even more insidious than the technological problem, there is the human problem.
Even if we have the technology to perfectly align the AI, what if humanity can't agree on what it should do?
[AI is already used in military applications](https://www.palantir.com/), where reasonable humans can have very different ethical views.

<div style="text-align: center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/XEM5qz__HOU?si=B_p0FNM_AVioCUuM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

What I take most from reading the series is additional awareness about what to focus on.
The starting point for any decision about technology is how it can be good for humanity.
In the context of AI tools, I don't want to give up the discussion about AI policy in name ot technological development for the sake of it.
If the political and market incentives are not going in the right direction, then it is up to us to steer them properly.

> A note of pessimism, followed by cautious optimism.
>
> Consider the last few decades in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? "Humans coordinating to deal with global threats" is... not a thing we seem to be > good at.
>
> But we used to be good at it! We eradicated smallpox, it's no longer the case that half of kids died before age 15[84], the ozone layer is actually > healing!
>
> Humans have solved the "human alignment problem" before.
>
> Let's get our groove back, and align ourselves on aligning AI.
> 
> [AI Safety for Fleshy Humans - part 3](https://aisafety.dance/p3)


## Footnotes
